{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:17.079838Z",
     "iopub.status.busy": "2025-01-24T23:56:17.079506Z",
     "iopub.status.idle": "2025-01-24T23:56:21.100368Z",
     "shell.execute_reply": "2025-01-24T23:56:21.099299Z",
     "shell.execute_reply.started": "2025-01-24T23:56:17.079813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset ,WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:21.101929Z",
     "iopub.status.busy": "2025-01-24T23:56:21.101540Z",
     "iopub.status.idle": "2025-01-24T23:56:21.106345Z",
     "shell.execute_reply": "2025-01-24T23:56:21.105403Z",
     "shell.execute_reply.started": "2025-01-24T23:56:21.101904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = (180, 180)\n",
    "EPOCHS = 15\n",
    "GCS_PATH = \"/kaggle/input/labeled-chest-xray-images/chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:21.330157Z",
     "iopub.status.busy": "2025-01-24T23:56:21.329787Z",
     "iopub.status.idle": "2025-01-24T23:56:23.292179Z",
     "shell.execute_reply": "2025-01-24T23:56:23.291310Z",
     "shell.execute_reply.started": "2025-01-24T23:56:21.330127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get filenames for train and test directories\n",
    "train_filenames = []\n",
    "for root, _, files in os.walk(os.path.join(GCS_PATH, 'train')):\n",
    "    for file in files:\n",
    "        if file.endswith(('jpeg', 'jpg', 'png')):\n",
    "            train_filenames.append(os.path.join(root, file))\n",
    "\n",
    "test_filenames = []\n",
    "for root, _, files in os.walk(os.path.join(GCS_PATH, 'test')):\n",
    "    for file in files:\n",
    "        if file.endswith(('jpeg', 'jpg', 'png')):\n",
    "            test_filenames.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:23.731192Z",
     "iopub.status.busy": "2025-01-24T23:56:23.730743Z",
     "iopub.status.idle": "2025-01-24T23:56:23.737678Z",
     "shell.execute_reply": "2025-01-24T23:56:23.736677Z",
     "shell.execute_reply.started": "2025-01-24T23:56:23.731156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optionally, create a validation set from the train set (if desired)\n",
    "train_filenames, val_filenames = train_test_split(train_filenames, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:25.599866Z",
     "iopub.status.busy": "2025-01-24T23:56:25.599495Z",
     "iopub.status.idle": "2025-01-24T23:56:25.607355Z",
     "shell.execute_reply": "2025-01-24T23:56:25.606445Z",
     "shell.execute_reply.started": "2025-01-24T23:56:25.599839Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal images count in training set: 1090\n",
      "Pneumonia images count in training set: 3095\n"
     ]
    }
   ],
   "source": [
    "# Count classes for weighted sampling\n",
    "COUNT_NORMAL = len([f for f in train_filenames if \"NORMAL\" in f])\n",
    "COUNT_PNEUMONIA = len([f for f in train_filenames if \"PNEUMONIA\" in f])\n",
    "print(f\"Normal images count in training set: {COUNT_NORMAL}\")\n",
    "print(f\"Pneumonia images count in training set: {COUNT_PNEUMONIA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:32.240664Z",
     "iopub.status.busy": "2025-01-24T23:56:32.240312Z",
     "iopub.status.idle": "2025-01-24T23:56:32.244875Z",
     "shell.execute_reply": "2025-01-24T23:56:32.243725Z",
     "shell.execute_reply.started": "2025-01-24T23:56:32.240637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Class labels\n",
    "CLASS_NAMES = [\"NORMAL\", \"PNEUMONIA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:34.355046Z",
     "iopub.status.busy": "2025-01-24T23:56:34.354697Z",
     "iopub.status.idle": "2025-01-24T23:56:34.360828Z",
     "shell.execute_reply": "2025-01-24T23:56:34.359771Z",
     "shell.execute_reply.started": "2025-01-24T23:56:34.355000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = 1 if \"PNEUMONIA\" in file_path else 0\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:36.719844Z",
     "iopub.status.busy": "2025-01-24T23:56:36.719509Z",
     "iopub.status.idle": "2025-01-24T23:56:36.724481Z",
     "shell.execute_reply": "2025-01-24T23:56:36.723331Z",
     "shell.execute_reply.started": "2025-01-24T23:56:36.719818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:38.759941Z",
     "iopub.status.busy": "2025-01-24T23:56:38.759572Z",
     "iopub.status.idle": "2025-01-24T23:56:38.764533Z",
     "shell.execute_reply": "2025-01-24T23:56:38.763429Z",
     "shell.execute_reply.started": "2025-01-24T23:56:38.759909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = ChestXRayDataset(train_filenames, transform=transform)\n",
    "val_dataset = ChestXRayDataset(val_filenames, transform=transform)\n",
    "test_dataset = ChestXRayDataset(test_filenames, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:40.559940Z",
     "iopub.status.busy": "2025-01-24T23:56:40.559568Z",
     "iopub.status.idle": "2025-01-24T23:56:40.564561Z",
     "shell.execute_reply": "2025-01-24T23:56:40.563792Z",
     "shell.execute_reply.started": "2025-01-24T23:56:40.559909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Class weights for handling imbalance\n",
    "class_weights = torch.tensor([\n",
    "    1.0 / COUNT_NORMAL,\n",
    "    1.0 / COUNT_PNEUMONIA\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:42.769841Z",
     "iopub.status.busy": "2025-01-24T23:56:42.769512Z",
     "iopub.status.idle": "2025-01-24T23:56:42.791343Z",
     "shell.execute_reply": "2025-01-24T23:56:42.790422Z",
     "shell.execute_reply.started": "2025-01-24T23:56:42.769816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_weights = [\n",
    "    class_weights[1 if \"PNEUMONIA\" in f else 0].item() for f in train_filenames\n",
    "]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:45.267842Z",
     "iopub.status.busy": "2025-01-24T23:56:45.267482Z",
     "iopub.status.idle": "2025-01-24T23:56:45.272796Z",
     "shell.execute_reply": "2025-01-24T23:56:45.271688Z",
     "shell.execute_reply.started": "2025-01-24T23:56:45.267797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:47.131627Z",
     "iopub.status.busy": "2025-01-24T23:56:47.131247Z",
     "iopub.status.idle": "2025-01-24T23:56:47.139283Z",
     "shell.execute_reply": "2025-01-24T23:56:47.138388Z",
     "shell.execute_reply.started": "2025-01-24T23:56:47.131595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "def dense_block(in_features, out_features, dropout_rate):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(out_features),\n",
    "        nn.Dropout(dropout_rate)\n",
    "    )\n",
    "\n",
    "class ChestXRayModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChestXRayModel, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            conv_block(3, 16),\n",
    "            conv_block(16, 32),\n",
    "            conv_block(32, 64),\n",
    "            conv_block(64, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            conv_block(128, 256),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            dense_block(256 * (IMAGE_SIZE[0] // 32) * (IMAGE_SIZE[1] // 32), 512, 0.7),\n",
    "            dense_block(512, 128, 0.5),\n",
    "            dense_block(128, 64, 0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T23:56:50.009739Z",
     "iopub.status.busy": "2025-01-24T23:56:50.009395Z",
     "iopub.status.idle": "2025-01-24T23:56:50.231227Z",
     "shell.execute_reply": "2025-01-24T23:56:50.230144Z",
     "shell.execute_reply.started": "2025-01-24T23:56:50.009714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChestXRayModel(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (6): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=6400, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.7, inplace=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = ChestXRayModel()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T00:00:32.411706Z",
     "iopub.status.busy": "2025-01-25T00:00:32.411270Z",
     "iopub.status.idle": "2025-01-25T00:00:32.417825Z",
     "shell.execute_reply": "2025-01-25T00:00:32.416820Z",
     "shell.execute_reply.started": "2025-01-25T00:00:32.411674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChestXRayModel(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (6): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=6400, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.7, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define loss function, optimizer, and metrics\n",
    "criterion = nn.BCEWithLogitsLoss()  # Handles binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to compute and print metrics\n",
    "def compute_metrics(labels, preds):\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(labels, preds, class_names):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to visualize sample images from the dataset\n",
    "def visualize_samples(data_loader):\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for i in range(5):\n",
    "        image = images[i].permute(1, 2, 0).numpy()  # Convert to HWC format for display\n",
    "        ax = axes[i]\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(f\"Label: {labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# Learning rate scheduler (exponential decay)\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 ** (epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(0.01, 20)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=exponential_decay_fn)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize sample images from the training set\n",
    "def visualize_samples(data_loader):\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for i in range(5):\n",
    "        image = images[i].permute(1, 2, 0).numpy()  # Convert to HWC format for display\n",
    "        ax = axes[i]\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(f\"Label: {labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualization of sample images from the training set\n",
    "visualize_samples(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"from tqdm import tqdm\n",
    "\n",
    "# Training loop with tqdm for progress tracking\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Wrap the train_loader with tqdm for progress tracking\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as tepoch:\n",
    "            for inputs, labels in tepoch:\n",
    "                inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float32)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Calculate training accuracy\n",
    "                predicted = (outputs.squeeze() > 0.5).float()\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                total_train += labels.size(0)\n",
    "\n",
    "                # Update the tqdm progress bar with loss and accuracy\n",
    "                train_accuracy = 100 * correct_train / total_train\n",
    "                tepoch.set_postfix(loss=running_loss / (tepoch.n + 1), train_accuracy=train_accuracy)\n",
    "\n",
    "        # Training accuracy for the current epoch\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        evaluate(model, val_loader)\n",
    "\n",
    "# The evaluation function remains the same as previously defined\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training loop with tqdm for progress tracking\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    metrics_dict = {'precision': [], 'recall': [], 'f1': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as tepoch:\n",
    "            for inputs, labels in tepoch:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float()  # Cast labels to float\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Calculate training accuracy\n",
    "                predicted = (outputs.squeeze() > 0.5).float()\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                total_train += labels.size(0)\n",
    "\n",
    "                train_accuracy = 100 * correct_train / total_train\n",
    "                tepoch.set_postfix(loss=running_loss / (tepoch.n + 1), train_accuracy=train_accuracy)\n",
    "\n",
    "        # Calculate training accuracy and append loss\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # Validation evaluation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float()  # Cast labels to float\n",
    "                outputs = model(inputs)\n",
    "                val_loss = criterion(outputs.squeeze(), labels)\n",
    "                val_running_loss += val_loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                predicted = (outputs.squeeze() > 0.5).float()\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "                \n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        val_losses.append(val_running_loss / len(val_loader))\n",
    "        print(f\"Validation Loss: {val_running_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Compute additional metrics\n",
    "        precision, recall, f1 = compute_metrics(all_labels, all_preds)\n",
    "        metrics_dict['precision'].append(precision)\n",
    "        metrics_dict['recall'].append(recall)\n",
    "        metrics_dict['f1'].append(f1)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(all_labels, all_preds, CLASS_NAMES)\n",
    "\n",
    "    # Plot the training and validation losses\n",
    "    plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(epochs), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Precision, Recall, F1 Score over epochs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(epochs), metrics_dict['precision'], label=\"Precision\")\n",
    "    plt.plot(range(epochs), metrics_dict['recall'], label=\"Recall\")\n",
    "    plt.plot(range(epochs), metrics_dict['f1'], label=\"F1 Score\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Precision, Recall, F1 Score over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the model with updated loop\n",
    "train(model, train_loader, val_loader, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()  # Cast labels to float\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    # Optionally, print or plot confusion matrix\n",
    "    plot_confusion_matrix(all_labels, all_preds, CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate_test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, '/kaggle/working/model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 835414,
     "sourceId": 1426603,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
